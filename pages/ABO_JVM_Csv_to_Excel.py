import streamlit as st
import pandas as pd
import pymongo
from datetime import datetime
import os
import re
from dotenv import load_dotenv

# Chargement des variables d'environnement
load_dotenv()

# RÃ©cupÃ©ration des clÃ©s d'API et connexion MongoDB depuis les variables d'environnement
mongo_uri = os.getenv("MONGODB_URI") or st.secrets.get("MONGODB_URI")

def robust_date_conversion(date_series):
    """Fonction amÃ©liorÃ©e pour la conversion des dates avec gestion de multiples formats"""
    # Copie des donnÃ©es pour prÃ©server l'original
    cleaned_dates = date_series.astype(str).str.strip()
    
    # Liste des formats Ã  essayer (du plus spÃ©cifique au plus gÃ©nÃ©ral)
    date_formats = [
        '%Y-%m-%dT%H:%M:%S%z',  # Format ISO avec fuseau horaire
        '%Y-%m-%dT%H:%M:%S',     # Format ISO sans fuseau horaire
        '%Y-%m-%d %H:%M:%S%z',   # Format standard avec fuseau horaire
        '%Y-%m-%d %H:%M:%S',     # Format standard sans fuseau horaire
        '%Y-%m-%d',              # Date simple
        '%d/%m/%Y %H:%M:%S',     # Format europÃ©en avec heure
        '%d/%m/%Y',              # Format europÃ©en simple
        '%m/%d/%Y %H:%M:%S',     # Format US avec heure
        '%m/%d/%Y',              # Format US simple
    ]
    
    # Convertir les dates en utilisant pandas
    result = pd.to_datetime(cleaned_dates, errors='coerce', utc=True)
    
    # Pour les dates qui n'ont pas Ã©tÃ© converties, essayons les formats spÃ©cifiques
    mask = result.isna()
    if mask.any():
        problematic_dates = cleaned_dates[mask]
        st.write("âš ï¸ **Formats de dates problÃ©matiques dÃ©tectÃ©s :**", problematic_dates.unique())
        
        # Essayer chaque format pour les dates problÃ©matiques
        for date_format in date_formats:
            try:
                # Convertir uniquement les dates problÃ©matiques restantes
                still_na = result.isna()
                temp_result = pd.to_datetime(cleaned_dates[still_na], format=date_format, errors='coerce')
                # Mettre Ã  jour seulement les valeurs qui ont Ã©tÃ© converties
                result[still_na] = temp_result
                # Si toutes les dates sont converties, sortir de la boucle
                if not result.isna().any():
                    break
            except Exception as e:
                continue
    
    # Supprimer les fuseaux horaires pour uniformisation
    if result.dt.tz is not None:
        result = result.dt.tz_localize(None)
    
    return result

def load_from_mongodb():
    """Charge les donnÃ©es des abonnÃ©s YouTube depuis MongoDB"""
    if not mongo_uri:
        st.error("L'URI MongoDB n'est pas configurÃ©. Veuillez vÃ©rifier vos variables d'environnement.")
        return pd.DataFrame()
    
    try:
        # Connexion Ã  MongoDB
        client = pymongo.MongoClient(mongo_uri)
        db = client.get_database()
        collection = db.users  # Assurez-vous que c'est le bon nom de collection
        
        # RÃ©cupÃ©ration des utilisateurs avec uniquement les nouveaux champs
        users = list(collection.find({}, {
            "_id": 0,  # Exclusion de l'ID MongoDB
            "customerID": 1,
            "deliveryName": 1,
            "deliveryAddress1": 1, 
            "deliveryAddress2": 1,
            "deliveryZip": 1,
            "deliveryCity": 1,
            "deliveryProvinceCode": 1,
            "deliveryCountryCode": 1,
            "billingCountry": 1,
            "quantity": 1,
            "discordId": 1,
            "username": 1
        }))
        
        if not users:
            st.warning("Aucun abonnÃ© YouTube trouvÃ© dans MongoDB.")
            return pd.DataFrame()
        
        # Conversion en DataFrame pandas
        df_youtube = pd.DataFrame(users)
        
        # Filtrer les entrÃ©es de test Brice N Guessan dÃ¨s le chargement
        if 'deliveryName' in df_youtube.columns:
            name_pattern = r"Brice N'?Guessan"
            name_mask = df_youtube['deliveryName'].str.contains(name_pattern, case=False, na=False, regex=True)
            
            # VÃ©rification pour l'email si prÃ©sent
            email_mask = pd.Series(False, index=df_youtube.index)
            if 'username' in df_youtube.columns:
                email_mask = df_youtube['username'].str.contains('bnguessan@linkdigitalspirit.com', case=False, na=False)
            
            # Combiner les masques
            test_mask = name_mask | email_mask
            test_count = test_mask.sum()
            
            if test_count > 0:
                st.warning(f"âš ï¸ Suppression de {test_count} entrÃ©es de test (Brice N Guessan) des donnÃ©es YouTube.")
                df_youtube = df_youtube[~test_mask]
            
        # Uniformiser les noms de colonnes pour le format final attendu par les transporteurs
        column_mapping = {
            "customerID": "ID",
            "deliveryName": "Customer name",
            "deliveryAddress1": "Delivery address 1",
            "deliveryAddress2": "Delivery address 2",
            "deliveryZip": "Delivery zip",
            "deliveryCity": "Delivery city",
            "deliveryProvinceCode": "Delivery province code",
            "deliveryCountryCode": "Delivery country code",
            "billingCountry": "Billing country",
            "quantity": "Delivery interval count"
        }
        
        # Renommer les colonnes pour correspondre au format attendu
        df_youtube = df_youtube.rename(columns=column_mapping)
        
        # Ajouter les colonnes manquantes pour la compatibilitÃ©
        for col in column_mapping.values():
            if col not in df_youtube.columns:
                df_youtube[col] = None
        
        # Ajouter la colonne Source pour distinguer ces entrÃ©es
        df_youtube['Source'] = 'YouTube'
        df_youtube['Status'] = 'ACTIVE'  # Tous les abonnÃ©s YouTube sont considÃ©rÃ©s comme actifs
        df_youtube['Created at'] = datetime.now()  # Date actuelle comme date de crÃ©ation
        
        st.success(f"âœ… **{len(df_youtube)} abonnÃ©s YouTube rÃ©cupÃ©rÃ©s avec succÃ¨s !**")
        return df_youtube
        
    except Exception as e:
        st.error(f"Erreur lors de la connexion Ã  MongoDB: {e}")
        return pd.DataFrame()

# Fonction dÃ©diÃ©e Ã  la suppression des entrÃ©es de test
def remove_test_entries(df):
    """Supprime toutes les entrÃ©es liÃ©es Ã  Brice N Guessan et son email"""
    if df.empty:
        return df
    
    initial_count = len(df)
    
    # Pattern pour le nom
    name_pattern = r"Brice N'?Guessan"
    name_mask = df['Customer name'].str.contains(name_pattern, case=False, na=False, regex=True)
    
    # Pattern pour l'email - vÃ©rifier si la colonne existe
    email_mask = pd.Series(False, index=df.index)
    email_columns = ['Email', 'Customer email', 'email']
    for email_col in email_columns:
        if email_col in df.columns:
            col_mask = df[email_col].str.contains('bnguessan@linkdigitalspirit.com', case=False, na=False)
            email_mask = email_mask | col_mask
    
    # Combiner les masques
    test_mask = name_mask | email_mask
    test_count = test_mask.sum()
    
    if test_count > 0:
        # Pour le dÃ©bogage: afficher des exemples d'entrÃ©es qui seront supprimÃ©es
        st.write(f"ğŸ” **Suppression de {test_count} entrÃ©es de test dÃ©tectÃ©es:**")
        preview_cols = ['ID', 'Customer name']
        preview_cols.extend([col for col in email_columns if col in df.columns])
        st.dataframe(df[test_mask][preview_cols].head())
        
        # Supprimer les entrÃ©es de test
        df = df[~test_mask]
        st.info(f"â„¹ï¸ {test_count} entrÃ©es de test ont Ã©tÃ© supprimÃ©es.")
    
    return df

def process_csv(uploaded_files, include_youtube=False):
    """Lit et traite plusieurs fichiers CSV, avec option d'inclure les abonnÃ©s YouTube."""
    all_dataframes = []

    for csv_file in uploaded_files:
        try:
            df = pd.read_csv(csv_file)
            st.write(f"âœ… Fichier {csv_file.name} chargÃ© : {len(df)} lignes")
            
            # Supprimer les entrÃ©es de test immÃ©diatement aprÃ¨s le chargement
            df = remove_test_entries(df)
            st.write(f"âœ… AprÃ¨s suppression des entrÃ©es de test: {len(df)} lignes")
            
            all_dataframes.append(df)
        except Exception as e:
            st.error(f"âŒ Erreur lors du chargement de {csv_file.name}: {e}")
    
    if not all_dataframes:
        st.error("Aucun fichier CSV n'a pu Ãªtre chargÃ©.")
        return None, None

    # Fusionner tous les fichiers en un seul DataFrame
    df = pd.concat(all_dataframes, ignore_index=True)

    # VÃ©rifier tous les statuts uniques prÃ©sents
    unique_statuses = df['Status'].unique()
    st.write(f"ğŸ“Š **Statuts d'abonnement trouvÃ©s :** {', '.join(unique_statuses)}")

    # Compter les abonnements par statut
    status_counts = df['Status'].value_counts()
    st.write("ğŸ“Š **Nombre d'abonnements par statut avant filtrage :**")
    for status, count in status_counts.items():
        st.write(f"- {status}: {count}")

    # VÃ©rifier les colonnes requises
    required_columns = ['ID', 'Created at', 'Status', 'Next order date', 'Customer name']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        st.error(f"Colonnes manquantes dans les fichiers CSV: {', '.join(missing_columns)}")
        return None, None

    # ğŸ” Afficher un aperÃ§u des donnÃ©es brutes
    st.write("ğŸ” **AperÃ§u des donnÃ©es brutes :**")
    st.dataframe(df.head(5))

    # ğŸ§¹ Nettoyage et conversion des dates
    st.write("ğŸ§¹ **Nettoyage et conversion des dates...**")
    
    # Utiliser notre fonction robuste de conversion de dates
    original_count = len(df)
    df['Created at'] = robust_date_conversion(df['Created at'])
    
    # Supprimer les lignes avec des dates invalides
    df = df.dropna(subset=['Created at'])
    if len(df) < original_count:
        st.warning(f"âš ï¸ {original_count - len(df)} lignes avec des dates invalides ont Ã©tÃ© supprimÃ©es.")

    # ğŸ” Afficher un aperÃ§u aprÃ¨s conversion
    st.write("ğŸ” **AperÃ§u aprÃ¨s conversion des dates :**")
    st.dataframe(df[['ID', 'Created at']].head(5))

    # ğŸš€ Filtrage : Suppression des abonnements crÃ©Ã©s aprÃ¨s le 5 du mois
    today = datetime.today()
    start_date = datetime(today.year, today.month, 5)

    df_before_filtering = len(df)

    # Ajoutez ce code pour capturer les abonnements qui vont Ãªtre supprimÃ©s
    df_to_be_removed = df[df['Created at'] >= start_date].copy()
    st.write(f"ğŸ” **Analyse des {len(df_to_be_removed)} abonnements qui seront supprimÃ©s :**")

    # Afficher un Ã©chantillon de ces abonnements pour analyse
    if not df_to_be_removed.empty:
        # Afficher les statistiques sur les statuts
        status_counts = df_to_be_removed['Status'].value_counts()
        st.write("RÃ©partition par statut :")
        for status, count in status_counts.items():
            st.write(f"- {status}: {count}")
        
        # Afficher les premiÃ¨res lignes pour inspection
        st.write("Ã‰chantillon des abonnements supprimÃ©s :")
        st.dataframe(df_to_be_removed[['ID', 'Status', 'Created at', 'Customer name']].head(10))
        
        # Option pour tÃ©lÃ©charger le fichier complet des supprimÃ©s
        removed_csv = df_to_be_removed.to_csv(index=False)
        st.download_button(
            label="ğŸ“¥ TÃ©lÃ©charger la liste des abonnements supprimÃ©s",
            data=removed_csv,
            file_name="abonnements_supprimes.csv",
            mime="text/csv"
        )

    # Continuer avec le filtrage normal
    df = df[df['Created at'] < start_date]
    df_after_filtering = len(df)

    st.write(f"ğŸš€ **Avant le filtrage : {df_before_filtering} abonnements**")
    st.write(f"âœ… **AprÃ¨s le filtrage : {df_after_filtering} abonnements (supprimÃ©s : {df_before_filtering - df_after_filtering})**")

    # Standardisation de la colonne 'Status'
    df['Status'] = df['Status'].str.upper()

    # Ajout de la colonne 'Source' pour Shopify
    df['Source'] = 'Shopify'

    # SÃ©paration initiale des abonnements par statut
    active_df = df[df['Status'] == 'ACTIVE']
    paused_df = df[df['Status'] == 'PAUSED']
    cancelled_df = df[df['Status'] == 'CANCELLED']

    # VÃ©rifier Ã  nouveau pour les entrÃ©es de test Ã  chaque Ã©tape
    active_df = remove_test_entries(active_df)
    cancelled_df = remove_test_entries(cancelled_df)

    # Afficher le nombre d'abonnements par statut
    st.write("ğŸ“Š **DÃ©tail des abonnements par statut :**")
    st.write(f"- ACTIVE: {len(active_df)}")
    st.write(f"- PAUSED: {len(paused_df)}")
    st.write(f"- CANCELLED: {len(cancelled_df)}")

    # Filtrage spÃ©cifique pour les abonnements annulÃ©s
    # D'abord, exclure les abonnements remboursÃ©s
    if 'Cancellation note' in cancelled_df.columns:
        # Compter les abonnements remboursÃ©s
        refund_mask = cancelled_df['Cancellation note'].str.contains('Remboursement', case=False, na=False)
        refund_count = refund_mask.sum()
        
        if refund_count > 0:
            # Afficher les abonnements remboursÃ©s qui seront exclus
            st.write(f"ğŸ” **Exclusion de {refund_count} abonnements annulÃ©s avec mention 'Remboursement' :**")
            st.dataframe(cancelled_df[refund_mask][['ID', 'Customer name', 'Cancellation note']].head(10))
            
            # Filtrer pour ne garder que les abonnements non remboursÃ©s
            cancelled_df = cancelled_df[~refund_mask]
            st.info(f"â„¹ï¸ {refund_count} abonnements remboursÃ©s ont Ã©tÃ© exclus.")
    
    # Ensuite, continuer avec le filtrage par date
    try:
        # Convertir la colonne Next order date sans fuseau horaire
        cancelled_df['Next order date'] = pd.to_datetime(cancelled_df['Next order date'], errors='coerce', utc=True)
        
        # S'assurer que les dates n'ont pas de fuseau horaire
        if cancelled_df['Next order date'].dt.tz is not None:
            cancelled_df['Next order date'] = cancelled_df['Next order date'].dt.tz_localize(None)
        
        # DÃ©terminer le 5 du mois actuel
        today = datetime.today()
        cutoff_date = datetime(today.year, today.month, 5)
        
        # Filtrer les abonnements annulÃ©s qui ont une date de prochaine commande aprÃ¨s le 5 du mois
        # OU qui ont une date de prochaine commande nulle/invalide (les garder par prÃ©caution)
        valid_cancelled_df = cancelled_df[cancelled_df['Next order date'] > cutoff_date]
        
        excluded_count = len(cancelled_df) - len(valid_cancelled_df)
        if excluded_count > 0:
            st.info(f"â„¹ï¸ {excluded_count} abonnements annulÃ©s ont Ã©tÃ© exclus car leur prochaine date de commande est avant le 5 du mois.")
        
        st.write(f"â„¹ï¸ Sur {len(cancelled_df)} abonnements annulÃ©s, {len(valid_cancelled_df)} ont une date de commande postÃ©rieure au 5 du mois et sont conservÃ©s.")
    except Exception as e:
        st.error(f"Erreur lors de l'analyse des dates de commande: {e}")
        valid_cancelled_df = cancelled_df.copy()  # En cas d'erreur, garder tous les abonnements annulÃ©s par prÃ©caution
    
    # VÃ©rifier Ã  nouveau pour les entrÃ©es de test dans les cancelled valides
    valid_cancelled_df = remove_test_entries(valid_cancelled_df)

    # IntÃ©grer les abonnÃ©s YouTube si demandÃ©
    if include_youtube:
        youtube_df = load_from_mongodb()
        if not youtube_df.empty:
            # Ajouter les abonnÃ©s YouTube aux abonnÃ©s actifs
            active_df = pd.concat([active_df, youtube_df], ignore_index=True)
            
            # VÃ©rifier une derniÃ¨re fois aprÃ¨s la fusion
            active_df = remove_test_entries(active_df)
            
            st.success(f"âœ… **{len(youtube_df)} abonnÃ©s YouTube ajoutÃ©s aux abonnÃ©s actifs !**")
    
    return active_df, valid_cancelled_df

# Interface utilisateur Streamlit
st.title("Gestion des abonnements JV Magazine")
st.write("ğŸ‘‹ Cet outil permet de traiter les abonnements Shopify et d'intÃ©grer les abonnÃ©s YouTube Discord.")

# Option pour inclure les abonnÃ©s YouTube
include_youtube = st.checkbox("Inclure les abonnÃ©s YouTube depuis MongoDB", value=False)

file_prefix = st.text_input("Entrez le prÃ©fixe pour les fichiers finaux :", "")
uploaded_files = st.file_uploader("TÃ©lÃ©versez les fichiers CSV des abonnements", type="csv", accept_multiple_files=True)

if uploaded_files:
    with st.spinner("Traitement des donnÃ©es en cours..."):
        active_df, cancelled_df = process_csv(uploaded_files, include_youtube)

    if active_df is not None and cancelled_df is not None:
        # Afficher les statistiques
        st.write("## ğŸ“Š Statistiques")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Abonnements actifs", len(active_df))
            if include_youtube:
                youtube_count = sum(active_df['Source'] == 'YouTube')
                shopify_count = sum(active_df['Source'] == 'Shopify')
                st.write(f"- Shopify: {shopify_count}")
                st.write(f"- YouTube: {youtube_count}")
        
        with col2:
            st.metric("Abonnements annulÃ©s", len(cancelled_df))
        
        # Afficher les aperÃ§us
        st.write("## ğŸ“‹ AperÃ§u des donnÃ©es")
        
        st.write(f"ğŸ“Œ **Abonnements actifs ({len(active_df)} lignes) :**")
        st.dataframe(active_df)

        st.write(f"ğŸ“Œ **Abonnements annulÃ©s ({len(cancelled_df)} lignes) :**")
        st.dataframe(cancelled_df)
        
        # Option d'exportation
        st.write("## ğŸ’¾ Exportation des donnÃ©es")

        # Fonction pour dÃ©terminer si une adresse est en France
        def is_france(row):
            # VÃ©rifier d'abord le pays de livraison s'il existe
            if 'Delivery country code' in row and pd.notna(row['Delivery country code']):
                return row['Delivery country code'].upper() == 'FR'
            
            # VÃ©rifier le pays de livraison s'il existe
            if 'Delivery country' in row and pd.notna(row['Delivery country']):
                return 'FRANCE' in row['Delivery country'].upper()
            
            # VÃ©rifier la mÃ©thode de livraison s'il existe
            if 'Delivery Method' in row and pd.notna(row['Delivery Method']):
                return 'FR' in row['Delivery Method'].upper()
            
            # Par dÃ©faut, considÃ©rer comme Ã©tranger si on ne peut pas dÃ©terminer
            return False

        # Fonction pour prÃ©parer le format final
        def prepare_final_files(df):
            """PrÃ©pare le fichier final avec les colonnes nÃ©cessaires et renommÃ©es."""
            column_mapping = {
                "ID": "Customer ID",
                "Customer name": "Delivery name",
                "Delivery address 1": "Delivery address 1",
                "Delivery address 2": "Delivery address 2",
                "Delivery zip": "Delivery zip",
                "Delivery city": "Delivery city",
                "Delivery province code": "Delivery province code",
                "Delivery country code": "Delivery country code",
                "Billing country": "Billing country",
                "Delivery interval count": "Quantity"
            }
            
            # S'assurer que toutes les colonnes nÃ©cessaires existent
            for col in column_mapping.keys():
                if col not in df.columns:
                    df[col] = None  # Ajouter une colonne vide si elle n'existe pas
            
            # SÃ©lectionner et renommer les colonnes
            df = df[list(column_mapping.keys())].rename(columns=column_mapping)
            
            # Remplir la colonne Billing country pour la France si elle est vide
            df['Billing country'] = df.apply(
                lambda row: "FRANCE" if pd.isnull(row['Billing country']) and row['Delivery country code'] == "FR" else row['Billing country'],
                axis=1
            )
            
            final_columns = [
                "Customer ID", "Delivery name", "Delivery address 1", "Delivery address 2",
                "Delivery zip", "Delivery city", "Delivery province code",
                "Delivery country code", "Billing country", "Quantity"
            ]
            return df[final_columns]

        # CrÃ©er les dataframes sÃ©parÃ©s pour France et Ã‰tranger
        if active_df is not None and cancelled_df is not None:
            # S'assurer que valid_cancelled_df existe
            if 'valid_cancelled_df' not in locals() and 'valid_cancelled_df' not in globals():
                valid_cancelled_df = cancelled_df
    
            # Combiner actifs et annulÃ©s
            all_df = pd.concat([active_df, valid_cancelled_df], ignore_index=True)
            
            # VÃ©rification finale pour les entrÃ©es de test
            all_df = remove_test_entries(all_df)
            
            # PrÃ©parer le format final
            all_df = prepare_final_files(all_df)

            # VÃ©rification ultime des entrÃ©es de test
            name_pattern = r"Brice N'?Guessan"
            mask = all_df['Delivery name'].str.contains(name_pattern, case=False, na=False, regex=True)
            test_count = mask.sum()
            if test_count > 0:
                st.warning(f"âš ï¸ {test_count} abonnements 'Brice N Guessan' ont Ã©tÃ© dÃ©tectÃ©s aprÃ¨s la prÃ©paration des donnÃ©es finales!")
                st.dataframe(all_df[mask][['Customer ID', 'Delivery name']].head())
                # Les supprimer
                all_df = all_df[~mask]
                st.info(f"â„¹ï¸ {test_count} abonnements de test 'Brice N Guessan' supprimÃ©s des donnÃ©es finales.")
            
            # SÃ©parer par pays
            france_df = all_df[all_df.apply(is_france, axis=1)]
            etranger_df = all_df[~all_df.apply(is_france, axis=1)]
            
            st.write(f"ğŸ“Š **RÃ©partition des abonnements :**")
            st.write(f"- France : {len(france_df)} abonnements")
            st.write(f"- Ã‰tranger : {len(etranger_df)} abonnements")
            
            # Afficher un aperÃ§u
            st.write(f"ğŸ“Œ **DonnÃ©es finales pour la France :**")
            st.dataframe(france_df)

            st.write(f"ğŸ“Œ **DonnÃ©es finales pour l'Ã©tranger :**")
            st.dataframe(etranger_df)

        # Colonnes d'export 
        col1, col2 = st.columns(2)

        with col1:
            st.write("### ğŸ‡«ğŸ‡· Abonnements France")
            if st.button("Exporter le fichier France"):
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # DÃ©finir le nom du fichier
                fr_filename = f"{file_prefix}_france_{timestamp}.csv" if file_prefix else f"france_{timestamp}.csv"
                
                # Convertir le DataFrame en CSV
                fr_csv = france_df.to_csv(index=False)
                
                # Proposer le tÃ©lÃ©chargement
                st.download_button(
                    label="ğŸ“¥ TÃ©lÃ©charger les abonnements France",
                    data=fr_csv,
                    file_name=fr_filename,
                    mime="text/csv"
                )
                
                st.success(f"âœ… Fichier France prÃªt Ã  Ãªtre tÃ©lÃ©chargÃ© ({len(france_df)} abonnements)")

        with col2:
            st.write("### ğŸŒ Abonnements Ã‰tranger")
            if st.button("Exporter le fichier Ã‰tranger"):
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # DÃ©finir le nom du fichier
                int_filename = f"{file_prefix}_etranger_{timestamp}.csv" if file_prefix else f"etranger_{timestamp}.csv"
                
                # Convertir le DataFrame en CSV
                int_csv = etranger_df.to_csv(index=False)
                
                # Proposer le tÃ©lÃ©chargement
                st.download_button(
                    label="ğŸ“¥ TÃ©lÃ©charger les abonnements Ã‰tranger",
                    data=int_csv,
                    file_name=int_filename,
                    mime="text/csv"
                )
                
                st.success(f"âœ… Fichier Ã‰tranger prÃªt Ã  Ãªtre tÃ©lÃ©chargÃ© ({len(etranger_df)} abonnements)")