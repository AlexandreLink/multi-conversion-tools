import streamlit as st
import pandas as pd
import pymongo
from datetime import datetime
import os
import re
from dotenv import load_dotenv

# Chargement des variables d'environnement
load_dotenv()

# R√©cup√©ration des cl√©s d'API et connexion MongoDB depuis les variables d'environnement
mongo_uri = os.getenv("MONGODB_URI") or st.secrets.get("MONGODB_URI")

def robust_date_conversion(date_series):
    """Fonction am√©lior√©e pour la conversion des dates avec gestion de multiples formats"""
    # Copie des donn√©es pour pr√©server l'original
    cleaned_dates = date_series.astype(str).str.strip()
    
    # Liste des formats √† essayer (du plus sp√©cifique au plus g√©n√©ral)
    date_formats = [
        '%Y-%m-%dT%H:%M:%S%z',  # Format ISO avec fuseau horaire
        '%Y-%m-%dT%H:%M:%S',     # Format ISO sans fuseau horaire
        '%Y-%m-%d %H:%M:%S%z',   # Format standard avec fuseau horaire
        '%Y-%m-%d %H:%M:%S',     # Format standard sans fuseau horaire
        '%Y-%m-%d',              # Date simple
        '%d/%m/%Y %H:%M:%S',     # Format europ√©en avec heure
        '%d/%m/%Y',              # Format europ√©en simple
        '%m/%d/%Y %H:%M:%S',     # Format US avec heure
        '%m/%d/%Y',              # Format US simple
    ]
    
    # Convertir les dates en utilisant pandas
    result = pd.to_datetime(cleaned_dates, errors='coerce', utc=True)
    
    # Pour les dates qui n'ont pas √©t√© converties, essayons les formats sp√©cifiques
    mask = result.isna()
    if mask.any():
        problematic_dates = cleaned_dates[mask]
        st.write("‚ö†Ô∏è **Formats de dates probl√©matiques d√©tect√©s :**", problematic_dates.unique())
        
        # Essayer chaque format pour les dates probl√©matiques
        for date_format in date_formats:
            try:
                # Convertir uniquement les dates probl√©matiques restantes
                still_na = result.isna()
                temp_result = pd.to_datetime(cleaned_dates[still_na], format=date_format, errors='coerce')
                # Mettre √† jour seulement les valeurs qui ont √©t√© converties
                result[still_na] = temp_result
                # Si toutes les dates sont converties, sortir de la boucle
                if not result.isna().any():
                    break
            except Exception as e:
                continue
    
    # Supprimer les fuseaux horaires pour uniformisation
    if result.dt.tz is not None:
        result = result.dt.tz_localize(None)
    
    return result

def load_from_mongodb():
    """Charge les donn√©es des abonn√©s YouTube depuis MongoDB"""
    if not mongo_uri:
        st.error("L'URI MongoDB n'est pas configur√©. Veuillez v√©rifier vos variables d'environnement.")
        return pd.DataFrame()
    
    try:
        # Connexion √† MongoDB
        client = pymongo.MongoClient(mongo_uri)
        db = client.get_database()
        collection = db.users  # Assurez-vous que c'est le bon nom de collection
        
        # R√©cup√©ration des utilisateurs
        users = list(collection.find({}, {
            "_id": 0,  # Exclusion de l'ID MongoDB
            "name": 1,
            "address": 1,
            "city": 1,
            "postalCode": 1,
            "country": 1,
            "discordId": 1,
            "username": 1
        }))
        
        if not users:
            st.warning("Aucun abonn√© YouTube trouv√© dans MongoDB.")
            return pd.DataFrame()
        
        # Conversion en DataFrame pandas
        df_youtube = pd.DataFrame(users)
        
        # Ajouter des colonnes pour harmoniser avec le format des abonnements Shopify
        df_youtube['Status'] = 'ACTIVE'  # Tous les abonn√©s YouTube sont consid√©r√©s comme actifs
        df_youtube['Source'] = 'YouTube'  # Marquer la source
        df_youtube['ID'] = df_youtube['discordId']  # Utiliser discordId comme identifiant unique
        df_youtube['Customer name'] = df_youtube['name']
        df_youtube['Created at'] = datetime.now()  # Date actuelle comme date de cr√©ation
        
        # Cr√©er une colonne d'adresse compl√®te
        df_youtube['Shipping address'] = df_youtube.apply(
            lambda row: f"{row.get('address', '')}, {row.get('city', '')}, {row.get('postalCode', '')}, {row.get('country', '')}",
            axis=1
        )
        
        st.success(f"‚úÖ **{len(df_youtube)} abonn√©s YouTube r√©cup√©r√©s avec succ√®s !**")
        return df_youtube
        
    except Exception as e:
        st.error(f"Erreur lors de la connexion √† MongoDB: {e}")
        return pd.DataFrame()

def process_csv(uploaded_files, include_youtube=False):
    """Lit et traite plusieurs fichiers CSV, avec option d'inclure les abonn√©s YouTube."""
    all_dataframes = []

    for csv_file in uploaded_files:
        try:
            df = pd.read_csv(csv_file)
            st.write(f"‚úÖ Fichier {csv_file.name} charg√© : {len(df)} lignes")
            all_dataframes.append(df)
        except Exception as e:
            st.error(f"‚ùå Erreur lors du chargement de {csv_file.name}: {e}")
    
    if not all_dataframes:
        st.error("Aucun fichier CSV n'a pu √™tre charg√©.")
        return None, None

    # Fusionner tous les fichiers en un seul DataFrame
    df = pd.concat(all_dataframes, ignore_index=True)

    # V√©rifier les colonnes requises
    required_columns = ['ID', 'Created at', 'Status', 'Next order date', 'Customer name']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        st.error(f"Colonnes manquantes dans les fichiers CSV: {', '.join(missing_columns)}")
        return None, None

    # üîç Afficher un aper√ßu des donn√©es brutes
    st.write("üîç **Aper√ßu des donn√©es brutes :**")
    st.dataframe(df.head(5))

    # üßπ Nettoyage et conversion des dates
    st.write("üßπ **Nettoyage et conversion des dates...**")
    
    # Utiliser notre fonction robuste de conversion de dates
    original_count = len(df)
    df['Created at'] = robust_date_conversion(df['Created at'])
    
    # Supprimer les lignes avec des dates invalides
    df = df.dropna(subset=['Created at'])
    if len(df) < original_count:
        st.warning(f"‚ö†Ô∏è {original_count - len(df)} lignes avec des dates invalides ont √©t√© supprim√©es.")

    # üîé Afficher un aper√ßu apr√®s conversion
    st.write("üîé **Aper√ßu apr√®s conversion des dates :**")
    st.dataframe(df[['ID', 'Created at']].head(5))

    # üöÄ Filtrage : Suppression des abonnements cr√©√©s apr√®s le 5 du mois
    today = datetime.today()
    start_date = datetime(today.year, today.month, 5)

    df_before_filtering = len(df)
    df = df[df['Created at'] < start_date]  # Comparaison entre dates maintenant uniformis√©es
    df_after_filtering = len(df)

    st.write(f"üöÄ **Avant le filtrage : {df_before_filtering} abonnements**")
    st.write(f"‚úÖ **Apr√®s le filtrage : {df_after_filtering} abonnements (supprim√©s : {df_before_filtering - df_after_filtering})**")

    # Standardisation de la colonne 'Status'
    df['Status'] = df['Status'].str.upper()

    # Ajout de la colonne 'Source' pour Shopify
    df['Source'] = 'Shopify'

    # Filtrage des abonnements actifs et annul√©s
    active_df = df[df['Status'] == 'ACTIVE']
    cancelled_df = df[df['Status'] == 'CANCELLED']

    # Supprimer les abonnements test (Brice N Guessan / Brice N'Guessan)
    pattern = r"Brice N'?Guessan"
    mask = cancelled_df['Customer name'].str.contains(pattern, case=False, na=False, regex=True)
    test_count = mask.sum()
    if test_count > 0:
        cancelled_df = cancelled_df[~mask]
        st.info(f"‚ÑπÔ∏è {test_count} abonnements de test ont √©t√© supprim√©s.")

    # Int√©grer les abonn√©s YouTube si demand√©
    if include_youtube:
        youtube_df = load_from_mongodb()
        if not youtube_df.empty:
            # S√©lectionner uniquement les colonnes n√©cessaires pour la fusion
            youtube_columns = [
                'ID', 'Customer name', 'Shipping address', 'Status', 
                'Created at', 'Source'
            ]
            youtube_df_filtered = youtube_df[youtube_columns]
            
            # Ajouter les abonn√©s YouTube aux abonn√©s actifs
            active_df = pd.concat([active_df, youtube_df_filtered], ignore_index=True)
            
            st.success(f"‚úÖ **{len(youtube_df)} abonn√©s YouTube ajout√©s aux abonn√©s actifs !**")

    return active_df, cancelled_df

# Interface utilisateur Streamlit
st.title("Gestion des abonnements JV Magazine")
st.write("üëã Cet outil permet de traiter les abonnements Shopify et d'int√©grer les abonn√©s YouTube Discord.")

# Option pour inclure les abonn√©s YouTube
include_youtube = st.checkbox("Inclure les abonn√©s YouTube depuis MongoDB", value=False)

file_prefix = st.text_input("Entrez le pr√©fixe pour les fichiers finaux :", "")
uploaded_files = st.file_uploader("T√©l√©versez les fichiers CSV des abonnements", type="csv", accept_multiple_files=True)

if uploaded_files:
    with st.spinner("Traitement des donn√©es en cours..."):
        active_df, cancelled_df = process_csv(uploaded_files, include_youtube)

    if active_df is not None and cancelled_df is not None:
        # Afficher les statistiques
        st.write("## üìä Statistiques")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Abonnements actifs", len(active_df))
            if include_youtube:
                youtube_count = sum(active_df['Source'] == 'YouTube')
                shopify_count = sum(active_df['Source'] == 'Shopify')
                st.write(f"- Shopify: {shopify_count}")
                st.write(f"- YouTube: {youtube_count}")
        
        with col2:
            st.metric("Abonnements annul√©s", len(cancelled_df))
        
        # Afficher les aper√ßus
        st.write("## üìã Aper√ßu des donn√©es")
        
        st.write(f"üìå **Abonnements actifs ({len(active_df)} lignes) :**")
        st.dataframe(active_df)

        st.write(f"üìå **Abonnements annul√©s ({len(cancelled_df)} lignes) :**")
        st.dataframe(cancelled_df)
        
        # Option d'exportation
        st.write("## üíæ Exportation des donn√©es")
        
        if st.button("Exporter les fichiers CSV"):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # D√©finir les noms de fichiers
            active_filename = f"{file_prefix}_actifs_{timestamp}.csv" if file_prefix else f"actifs_{timestamp}.csv"
            cancelled_filename = f"{file_prefix}_annules_{timestamp}.csv" if file_prefix else f"annules_{timestamp}.csv"
            
            # Convertir les DataFrames en CSV
            active_csv = active_df.to_csv(index=False)
            cancelled_csv = cancelled_df.to_csv(index=False)
            
            # Proposer le t√©l√©chargement
            st.download_button(
                label="üì• T√©l√©charger les abonnements actifs",
                data=active_csv,
                file_name=active_filename,
                mime="text/csv"
            )
            
            st.download_button(
                label="üì• T√©l√©charger les abonnements annul√©s",
                data=cancelled_csv,
                file_name=cancelled_filename,
                mime="text/csv"
            )
            
            st.success("‚úÖ Fichiers pr√™ts √† √™tre t√©l√©charg√©s !")