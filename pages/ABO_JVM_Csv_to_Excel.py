import streamlit as st
import pandas as pd
import pymongo
from datetime import datetime
import os
import re
from dotenv import load_dotenv

# Chargement des variables d'environnement
load_dotenv()

# R√©cup√©ration des cl√©s d'API et connexion MongoDB depuis les variables d'environnement
mongo_uri = os.getenv("MONGODB_URI") or st.secrets.get("MONGODB_URI")

def robust_date_conversion(date_series):
    """Fonction am√©lior√©e pour la conversion des dates avec gestion de multiples formats"""
    # Copie des donn√©es pour pr√©server l'original
    cleaned_dates = date_series.astype(str).str.strip()
    
    # Liste des formats √† essayer (du plus sp√©cifique au plus g√©n√©ral)
    date_formats = [
        '%Y-%m-%dT%H:%M:%S%z',  # Format ISO avec fuseau horaire
        '%Y-%m-%dT%H:%M:%S',     # Format ISO sans fuseau horaire
        '%Y-%m-%d %H:%M:%S%z',   # Format standard avec fuseau horaire
        '%Y-%m-%d %H:%M:%S',     # Format standard sans fuseau horaire
        '%Y-%m-%d',              # Date simple
        '%d/%m/%Y %H:%M:%S',     # Format europ√©en avec heure
        '%d/%m/%Y',              # Format europ√©en simple
        '%m/%d/%Y %H:%M:%S',     # Format US avec heure
        '%m/%d/%Y',              # Format US simple
    ]
    
    # Convertir les dates en utilisant pandas
    result = pd.to_datetime(cleaned_dates, errors='coerce', utc=True)
    
    # Pour les dates qui n'ont pas √©t√© converties, essayons les formats sp√©cifiques
    mask = result.isna()
    if mask.any():
        problematic_dates = cleaned_dates[mask]
        st.write("‚ö†Ô∏è **Formats de dates probl√©matiques d√©tect√©s :**", problematic_dates.unique())
        
        # Essayer chaque format pour les dates probl√©matiques
        for date_format in date_formats:
            try:
                # Convertir uniquement les dates probl√©matiques restantes
                still_na = result.isna()
                temp_result = pd.to_datetime(cleaned_dates[still_na], format=date_format, errors='coerce')
                # Mettre √† jour seulement les valeurs qui ont √©t√© converties
                result[still_na] = temp_result
                # Si toutes les dates sont converties, sortir de la boucle
                if not result.isna().any():
                    break
            except Exception as e:
                continue
    
    # Supprimer les fuseaux horaires pour uniformisation
    if result.dt.tz is not None:
        result = result.dt.tz_localize(None)
    
    return result

def load_from_mongodb():
    """Charge les donn√©es des abonn√©s YouTube depuis MongoDB"""
    if not mongo_uri:
        st.error("L'URI MongoDB n'est pas configur√©. Veuillez v√©rifier vos variables d'environnement.")
        return pd.DataFrame()
    
    try:
        # Connexion √† MongoDB
        client = pymongo.MongoClient(mongo_uri)
        db = client.get_database()
        collection = db.users  # Assurez-vous que c'est le bon nom de collection
        
        # R√©cup√©ration des utilisateurs
        users = list(collection.find({}, {
            "_id": 0,  # Exclusion de l'ID MongoDB
            "name": 1,
            "address": 1,
            "city": 1,
            "postalCode": 1,
            "country": 1,
            "discordId": 1,
            "username": 1
        }))
        
        if not users:
            st.warning("Aucun abonn√© YouTube trouv√© dans MongoDB.")
            return pd.DataFrame()
        
        # Conversion en DataFrame pandas
        df_youtube = pd.DataFrame(users)
        
        # Ajouter et renommer les colonnes pour correspondre au format attendu
        df_youtube['ID'] = df_youtube['discordId']  # Utiliser discordId comme identifiant unique
        df_youtube['Customer name'] = df_youtube['name']
        df_youtube['Delivery address 1'] = df_youtube['address']
        df_youtube['Delivery address 2'] = ""  # Colonne vide par d√©faut
        df_youtube['Delivery city'] = df_youtube['city']
        df_youtube['Delivery zip'] = df_youtube['postalCode']
        df_youtube['Delivery country code'] = df_youtube.apply(
            lambda row: "FR" if row.get('country', "").upper() == "FRANCE" else row.get('country', ""), 
            axis=1
        )
        df_youtube['Delivery province code'] = ""  # Colonne vide par d√©faut
        df_youtube['Billing country'] = df_youtube.apply(
            lambda row: "FRANCE" if row.get('country', "").upper() == "FRANCE" else row.get('country', ""),
            axis=1
        )
        df_youtube['Delivery interval count'] = 1  # Par d√©faut, 1 exemplaire par abonn√©
        df_youtube['Status'] = 'ACTIVE'  # Tous les abonn√©s YouTube sont consid√©r√©s comme actifs
        df_youtube['Source'] = 'YouTube'  # Marquer la source
        df_youtube['Created at'] = datetime.now()  # Date actuelle comme date de cr√©ation
        
        # Cr√©er une colonne d'adresse compl√®te pour affichage uniquement
        df_youtube['Shipping address'] = df_youtube.apply(
            lambda row: f"{row.get('address', '')}, {row.get('city', '')}, {row.get('postalCode', '')}, {row.get('country', '')}",
            axis=1
        )
        
        st.success(f"‚úÖ **{len(df_youtube)} abonn√©s YouTube r√©cup√©r√©s avec succ√®s !**")
        return df_youtube
        
    except Exception as e:
        st.error(f"Erreur lors de la connexion √† MongoDB: {e}")
        return pd.DataFrame()

def process_csv(uploaded_files, include_youtube=False):
    """Lit et traite plusieurs fichiers CSV, avec option d'inclure les abonn√©s YouTube."""
    all_dataframes = []

    for csv_file in uploaded_files:
        try:
            df = pd.read_csv(csv_file)
            st.write(f"‚úÖ Fichier {csv_file.name} charg√© : {len(df)} lignes")
            all_dataframes.append(df)
        except Exception as e:
            st.error(f"‚ùå Erreur lors du chargement de {csv_file.name}: {e}")
    
    if not all_dataframes:
        st.error("Aucun fichier CSV n'a pu √™tre charg√©.")
        return None, None

    # Fusionner tous les fichiers en un seul DataFrame
    df = pd.concat(all_dataframes, ignore_index=True)

    # V√©rifier tous les statuts uniques pr√©sents
    unique_statuses = df['Status'].unique()
    st.write(f"üìä **Statuts d'abonnement trouv√©s :** {', '.join(unique_statuses)}")

    # Compter les abonnements par statut
    status_counts = df['Status'].value_counts()
    st.write("üìä **Nombre d'abonnements par statut avant filtrage :**")
    for status, count in status_counts.items():
        st.write(f"- {status}: {count}")

    # V√©rifier les colonnes requises
    required_columns = ['ID', 'Created at', 'Status', 'Next order date', 'Customer name']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        st.error(f"Colonnes manquantes dans les fichiers CSV: {', '.join(missing_columns)}")
        return None, None

    # üîç Afficher un aper√ßu des donn√©es brutes
    st.write("üîç **Aper√ßu des donn√©es brutes :**")
    st.dataframe(df.head(5))

    # üßπ Nettoyage et conversion des dates
    st.write("üßπ **Nettoyage et conversion des dates...**")
    
    # Utiliser notre fonction robuste de conversion de dates
    original_count = len(df)
    df['Created at'] = robust_date_conversion(df['Created at'])
    
    # Supprimer les lignes avec des dates invalides
    df = df.dropna(subset=['Created at'])
    if len(df) < original_count:
        st.warning(f"‚ö†Ô∏è {original_count - len(df)} lignes avec des dates invalides ont √©t√© supprim√©es.")

    # üîé Afficher un aper√ßu apr√®s conversion
    st.write("üîé **Aper√ßu apr√®s conversion des dates :**")
    st.dataframe(df[['ID', 'Created at']].head(5))

    # üöÄ Filtrage : Suppression des abonnements cr√©√©s apr√®s le 5 du mois
    today = datetime.today()
    start_date = datetime(today.year, today.month, 5)

    df_before_filtering = len(df)

    # Ajoutez ce code pour capturer les abonnements qui vont √™tre supprim√©s
    df_to_be_removed = df[df['Created at'] >= start_date].copy()
    st.write(f"üîç **Analyse des {len(df_to_be_removed)} abonnements qui seront supprim√©s :**")

    # Afficher un √©chantillon de ces abonnements pour analyse
    if not df_to_be_removed.empty:
        # Afficher les statistiques sur les statuts
        status_counts = df_to_be_removed['Status'].value_counts()
        st.write("R√©partition par statut :")
        for status, count in status_counts.items():
            st.write(f"- {status}: {count}")
        
        # Afficher les premi√®res lignes pour inspection
        st.write("√âchantillon des abonnements supprim√©s :")
        st.dataframe(df_to_be_removed[['ID', 'Status', 'Created at', 'Customer name']].head(10))
        
        # Option pour t√©l√©charger le fichier complet des supprim√©s
        removed_csv = df_to_be_removed.to_csv(index=False)
        st.download_button(
            label="üì• T√©l√©charger la liste des abonnements supprim√©s",
            data=removed_csv,
            file_name="abonnements_supprimes.csv",
            mime="text/csv"
        )

    # Continuer avec le filtrage normal
    df = df[df['Created at'] < start_date]
    df_after_filtering = len(df)

    st.write(f"üöÄ **Avant le filtrage : {df_before_filtering} abonnements**")
    st.write(f"‚úÖ **Apr√®s le filtrage : {df_after_filtering} abonnements (supprim√©s : {df_before_filtering - df_after_filtering})**")

    # Standardisation de la colonne 'Status'
    df['Status'] = df['Status'].str.upper()

    # Ajout de la colonne 'Source' pour Shopify
    df['Source'] = 'Shopify'

    # S√©paration initiale des abonnements par statut
    active_df = df[df['Status'] == 'ACTIVE']
    paused_df = df[df['Status'] == 'PAUSED']
    cancelled_df = df[df['Status'] == 'CANCELLED']

    # Afficher le nombre d'abonnements par statut
    st.write("üìä **D√©tail des abonnements par statut :**")
    st.write(f"- ACTIVE: {len(active_df)}")
    st.write(f"- PAUSED: {len(paused_df)}")
    st.write(f"- CANCELLED: {len(cancelled_df)}")

    # Filtrage sp√©cifique pour les abonnements annul√©s
    # Conversion de "Next order date" en datetime avec gestion des erreurs
    try:
        # Convertir la colonne Next order date sans fuseau horaire
        cancelled_df['Next order date'] = pd.to_datetime(cancelled_df['Next order date'], errors='coerce', utc=True)
        
        # S'assurer que les dates n'ont pas de fuseau horaire
        if cancelled_df['Next order date'].dt.tz is not None:
            cancelled_df['Next order date'] = cancelled_df['Next order date'].dt.tz_localize(None)
        
        # D√©terminer le 5 du mois actuel
        today = datetime.today()
        cutoff_date = datetime(today.year, today.month, 5)
        
        # Filtrer les abonnements annul√©s qui ont une date de prochaine commande apr√®s le 5 du mois
        # OU qui ont une date de prochaine commande nulle/invalide (les garder par pr√©caution)
        valid_cancelled_df = cancelled_df[cancelled_df['Next order date'] > cutoff_date]
        
        excluded_count = len(cancelled_df) - len(valid_cancelled_df)
        if excluded_count > 0:
            st.info(f"‚ÑπÔ∏è {excluded_count} abonnements annul√©s ont √©t√© exclus car leur prochaine date de commande est avant le 5 du mois.")
        
        st.write(f"‚ÑπÔ∏è Sur {len(cancelled_df)} abonnements annul√©s, {len(valid_cancelled_df)} ont une date de commande post√©rieure au 5 du mois et sont conserv√©s.")
    except Exception as e:
        st.error(f"Erreur lors de l'analyse des dates de commande: {e}")
        valid_cancelled_df = cancelled_df.copy()  # En cas d'erreur, garder tous les abonnements annul√©s par pr√©caution
    
    # Supprimer les abonnements test (Brice N Guessan / Brice N'Guessan)
    pattern = r"Brice N'?Guessan"
    mask = valid_cancelled_df['Customer name'].str.contains(pattern, case=False, na=False, regex=True)
    test_count = mask.sum()
    if test_count > 0:
        valid_cancelled_df = valid_cancelled_df[~mask]
        st.info(f"‚ÑπÔ∏è {test_count} abonnements de test ont √©t√© supprim√©s.")

    # Int√©grer les abonn√©s YouTube si demand√©
    if include_youtube:
        youtube_df = load_from_mongodb()
        if not youtube_df.empty:
            # S√©lectionner uniquement les colonnes n√©cessaires pour la fusion
            youtube_columns = [
                'ID', 'Customer name', 'Shipping address', 'Status', 
                'Created at', 'Source'
            ]
            youtube_df_filtered = youtube_df[youtube_columns]
            
            # Ajouter les abonn√©s YouTube aux abonn√©s actifs
            active_df = pd.concat([active_df, youtube_df_filtered], ignore_index=True)
            
            st.success(f"‚úÖ **{len(youtube_df)} abonn√©s YouTube ajout√©s aux abonn√©s actifs !**")

    return active_df, valid_cancelled_df

# Interface utilisateur Streamlit
st.title("Gestion des abonnements JV Magazine")
st.write("üëã Cet outil permet de traiter les abonnements Shopify et d'int√©grer les abonn√©s YouTube Discord.")

# Option pour inclure les abonn√©s YouTube
include_youtube = st.checkbox("Inclure les abonn√©s YouTube depuis MongoDB", value=False)

file_prefix = st.text_input("Entrez le pr√©fixe pour les fichiers finaux :", "")
uploaded_files = st.file_uploader("T√©l√©versez les fichiers CSV des abonnements", type="csv", accept_multiple_files=True)

if uploaded_files:
    with st.spinner("Traitement des donn√©es en cours..."):
        active_df, cancelled_df = process_csv(uploaded_files, include_youtube)

    if active_df is not None and cancelled_df is not None:
        # Afficher les statistiques
        st.write("## üìä Statistiques")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Abonnements actifs", len(active_df))
            if include_youtube:
                youtube_count = sum(active_df['Source'] == 'YouTube')
                shopify_count = sum(active_df['Source'] == 'Shopify')
                st.write(f"- Shopify: {shopify_count}")
                st.write(f"- YouTube: {youtube_count}")
        
        with col2:
            st.metric("Abonnements annul√©s", len(cancelled_df))
        
        # Afficher les aper√ßus
        st.write("## üìã Aper√ßu des donn√©es")
        
        st.write(f"üìå **Abonnements actifs ({len(active_df)} lignes) :**")
        st.dataframe(active_df)

        st.write(f"üìå **Abonnements annul√©s ({len(cancelled_df)} lignes) :**")
        st.dataframe(cancelled_df)
        
        # Option d'exportation
        st.write("## üíæ Exportation des donn√©es")

        # Fonction pour d√©terminer si une adresse est en France
        def is_france(row):
            # V√©rifier d'abord le pays de livraison s'il existe
            if 'Delivery country code' in row and pd.notna(row['Delivery country code']):
                return row['Delivery country code'].upper() == 'FR'
            
            # V√©rifier le pays de livraison s'il existe
            if 'Delivery country' in row and pd.notna(row['Delivery country']):
                return 'FRANCE' in row['Delivery country'].upper()
            
            # V√©rifier la m√©thode de livraison s'il existe
            if 'Delivery Method' in row and pd.notna(row['Delivery Method']):
                return 'FR' in row['Delivery Method'].upper()
            
            # Par d√©faut, consid√©rer comme √©tranger si on ne peut pas d√©terminer
            return False

        # Fonction pour pr√©parer le format final
        def prepare_final_files(df):
            """Pr√©pare le fichier final avec les colonnes n√©cessaires et renomm√©es."""
            column_mapping = {
                "ID": "Customer ID",
                "Customer name": "Delivery name",
                "Delivery address 1": "Delivery address 1",
                "Delivery address 2": "Delivery address 2",
                "Delivery zip": "Delivery zip",
                "Delivery city": "Delivery city",
                "Delivery province code": "Delivery province code",
                "Delivery country code": "Delivery country code",
                "Billing country": "Billing country",
                "Delivery interval count": "Quantity"
            }
            
            # S'assurer que toutes les colonnes n√©cessaires existent
            for col in column_mapping.keys():
                if col not in df.columns:
                    df[col] = None  # Ajouter une colonne vide si elle n'existe pas
            
            # S√©lectionner et renommer les colonnes
            df = df[list(column_mapping.keys())].rename(columns=column_mapping)
            
            # Remplir la colonne Billing country pour la France si elle est vide
            df['Billing country'] = df.apply(
                lambda row: "FRANCE" if pd.isnull(row['Billing country']) and row['Delivery country code'] == "FR" else row['Billing country'],
                axis=1
            )
            
            final_columns = [
                "Customer ID", "Delivery name", "Delivery address 1", "Delivery address 2",
                "Delivery zip", "Delivery city", "Delivery province code",
                "Delivery country code", "Billing country", "Quantity"
            ]
            return df[final_columns]

        # Cr√©er les dataframes s√©par√©s pour France et √âtranger
        if active_df is not None and cancelled_df is not None:
            # Combiner actifs et annul√©s
            all_df = pd.concat([active_df, cancelled_df], ignore_index=True)
            
            # Pr√©parer le format final
            all_df = prepare_final_files(all_df)
            
            # S√©parer par pays
            france_df = all_df[all_df.apply(is_france, axis=1)]
            etranger_df = all_df[~all_df.apply(is_france, axis=1)]
            
            st.write(f"üìä **R√©partition des abonnements :**")
            st.write(f"- France : {len(france_df)} abonnements")
            st.write(f"- √âtranger : {len(etranger_df)} abonnements")
            
            # Afficher un aper√ßu
            st.write(f"üìå **Aper√ßu des donn√©es finales pour la France :**")
            st.dataframe(france_df.head(5))
            
            st.write(f"üìå **Aper√ßu des donn√©es finales pour l'√©tranger :**")
            st.dataframe(etranger_df.head(5))

        # Colonnes d'export 
        col1, col2 = st.columns(2)

        with col1:
            st.write("### üá´üá∑ Abonnements France")
            if st.button("Exporter le fichier France"):
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # D√©finir le nom du fichier
                fr_filename = f"{file_prefix}_france_{timestamp}.csv" if file_prefix else f"france_{timestamp}.csv"
                
                # Convertir le DataFrame en CSV
                fr_csv = france_df.to_csv(index=False)
                
                # Proposer le t√©l√©chargement
                st.download_button(
                    label="üì• T√©l√©charger les abonnements France",
                    data=fr_csv,
                    file_name=fr_filename,
                    mime="text/csv"
                )
                
                st.success(f"‚úÖ Fichier France pr√™t √† √™tre t√©l√©charg√© ({len(france_df)} abonnements)")

        with col2:
            st.write("### üåç Abonnements √âtranger")
            if st.button("Exporter le fichier √âtranger"):
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # D√©finir le nom du fichier
                int_filename = f"{file_prefix}_etranger_{timestamp}.csv" if file_prefix else f"etranger_{timestamp}.csv"
                
                # Convertir le DataFrame en CSV
                int_csv = etranger_df.to_csv(index=False)
                
                # Proposer le t√©l√©chargement
                st.download_button(
                    label="üì• T√©l√©charger les abonnements √âtranger",
                    data=int_csv,
                    file_name=int_filename,
                    mime="text/csv"
                )
                
                st.success(f"‚úÖ Fichier √âtranger pr√™t √† √™tre t√©l√©charg√© ({len(etranger_df)} abonnements)")